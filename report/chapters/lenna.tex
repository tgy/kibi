<<<<<<< HEAD
\chapter{Lenna -- \emph{Preprocessing}}
Avant d'essayer de détecter les paragraphes, les lignes ainsi que les caractères dans l'image, et de reconnaître les caractères, il faut "nettoyer" l'image. Dans ce chapitre nous allons décrire différents algorithmes que nous avons implémentés dans ce but.\\
=======
\chapter{Lenna - \emph{Preprocessing}}
>>>>>>> a594709738a33fb3f963fe16b8b134563e30681b

\section{Binarisation}
C'est le premier algorithme que nous avons écrit parce qu'il est simple, essentiel et c'est un bon point de départ pour se familiariser avec la bibliothèque et ses fonctions de base. Nous avons d'abord implémenté la méthode la plus naïve : calculer la moyenne des composantes RGB de chaque pixel pour avoir sa luminosité, et appliquer un seuil. Tout pixel dont la luminosité est en dessous de 127 est considéré comme blanc et les autres comme noir. Evidemment, ce ne fut pas notre algorithme définitif.\\

Nous aurions pu l'améliorer en prenant comme seuil la luminosité moyenne des pixels, cependant cela n'aurais pas été beaucoup mieux. Nous avons plutot décidé de rechercher un meilleur algorithme, et avons décidé que la méthode d'Otsu était la plus adaptée à notre cas. De plus cet algorithme est relativement simple à impémenter. La principe de la méthode d'Otsu est de trouver le seuil idéal pour l'algorithme décrit précedemment. Le seui idéal est défini comme étant celui qui minimise la variance entre les deux classes formées par ce seuil. Il faut donc essayer chaque seuil possible et calculer à chaque fois la variance de chaque classe. Heureusement une petite astuce mathématique nous permet de réduire de façon significative la complexité de cet algorithme en maximisant la variance inter-classe au lieu de minimiser la variance intra-classe.\\

\section{Réduction du bruit}

Nous avons éssayé deux méthodes pour réduire le bruit dans l'image : le flou gaussien et le filtre médian. Nous avons été insatisfait des deux car les lettres étaient moins lisible après le filtre. Pour corriger ça, nous avons réduit le nombre de pixel voisins pris en compte dans le filtre median : pour chaque pixel, nous prenons le pixel median parmi le pixel lui même et ses quatre voisins immédiats.\\

\section{Rotation}
\section{Algorithme de base}

<<<<<<< HEAD
Il est très difficile de détecter les zones de texte dans l'image si elle n'est pas droite. Nous avons donc des algorithme pour détecter l'angle du texte dans l'image, puis pour tourner l'image selon cet angle.\\
=======
Il est très difficile voir impossible (ou du moins contre-productif) de
détecter des zones de textes, caractères, mots dans une image si cette image
n'est pas droite. C'est pourquoi il est nécessaire, au cours de l'étape de
pré-traitement \emph{pre-processing}, de roter l'image fournie par l'utilisateur
afin qu'elle soit bien adaptée aux étapes de segmentation et d'identification
des caractères. \\

\subsection{Implémentation naive}

Une implémentation naive de la rotation serait l'algorithme suivant. C'est
celui que nous utilisions à la première soutenance mais qui posait quelques
soucis. \\

\begin{itemize}
  \item{Input} \\
    \begin{itemize}
      \item Image de base
      \item Angle de rotation $\theta$
    \end{itemize}
  \item{Output}
    \begin{itemize}
      \item Image rotée
    \end{itemize}
\end{itemize}

\begin{enumerate}

  \item Soit $(w, h)$ les dimensions de l'image de base. Calculer les dimensions
    $(nw, nh)$ de l'image après qu'une rotation d'angle $\theta$ soit appliquée
    sur l'image de base. Visuellement, les dimensions de l'image de base et de
    l'image rotée sont liées par une simple relation trigonométrique.
  \begin{center}
    \includegraphics[scale=0.75]{chapters/Pictures/toogy/rotation-new-size.eps}
  \end{center}

  \item Créer une nouvelle image de dimensions $(nw, nh)$ remplie de blanc.

  \item Pour chaque pixel de la nouvelle image, matérialisé par ses coordonnées
    $(x,y)$ dans l'image rotée, calculer ses anciennes coordonnées $(x',y')$
    dans l'image de base. Pour ce faire, on a la relation: \\ 
    $$\begin{pmatrix}x'\\y'\end{pmatrix} =
      \begin{pmatrix}
        \cos\theta & -\sin\theta \\
        \sin\theta & \cos\theta
      \end{pmatrix}
    \begin{pmatrix}
      x \\
      y
    \end{pmatrix}$$

  \item Arrondir ces nouvelles coordonnées et donner aux pixel $(x,y)$ de la
    nouvelle image la couleur du pixel $(x',y')$ de l'image de base. Renvoyer
    l'image.

\end{enumerate}

Cependant, cet algorithme déforme légèrement l'image. Ici, il s'agit de texte et
cette petite déformation réduit énormément sa lisibilité; pour l'homme comme
pour la machine. Il s'agit donc d'être plus intelligent.

\subsection{Interpolation Bilinéaire}

\subsubsection{Interpolation mathématique}

En mathématiques, une interpolation consiste à se servir de données déjà
existante pour approximer des données qui nous manquent. \\

Une interpolation peut se faire de plusieurs façons; plus ou moins précises.
Bien sûr, qui dit plus de précision dit plus de temps de calcul.

\subsubsection{Interpolation bilinéaire appliquée à la rotation}

Ici, il s'agit de se servir des pixels de notre image de base (nos données) pour
déterminer les pixels de notre nouvelle image (les données que nous allons
interpoler). \\

Plusieurs algorithmes d'interpolation existent :
\begin{itemize}
  \item{Nearest-neighbor Interpolation} : le plus simple, il consiste à faire
    la moyenne des composantes $(r,g,b)$ des pixels voisins de notre pixel
    $(x',y')$. C'est le plus rapide.
  \item{Bilinear Interpolation} : un peu plus complexe, il s'agit ici de faire
    deux interpolations linéaires : une entre les deux pixels au dessus de
    $(x',y')$ et une autre entre les deux pixels en dessous de $(x',y')$ (les
    valeurs $x'$ et $y'$ n'étant évidemment pas approximées mais bien située
    entre les entiers de l'arrondi supérieure et de l'arrondi inférieur des
    coordonnées).
    \begin{center}
      \includegraphics[scale=0.75]{chapters/Pictures/toogy/bilinear.eps}
    \end{center}
    On a alors : \\
    $$W_{1} = \frac{x_{2} - x}{x_{2} - x_{1}} N_{11}
    + \frac{x - x_{1}}{x_{2} - x_{1}} N_{21}$$ \\
    $$\text{et } W_{2} = \frac{x_{2} - x}{x_{2} - x_{1}} N_{12}
    + \frac{x - x_{1}}{x_{2} - x_{1}} N_{22}$$ \\
    Les nouvelles composantes du pixel peuvent alors être calculée via la
    formule \\
    $$R = \frac{y_{2} - y}{y_{2} - y_{1}} W_{1}
    + \frac{y - y_{1}}{y_{2} - y_{1}} W_{2} \text{ \emph{(cas de la
    composante rouge du pixel)}}$$
  \item{Bicubic Interpolation} : plus longue de part son temps de calcul, c'est
    le même principle que pour l'interpolatian bicubique sauf que 8 pixels sont
    étudiés au lieu de 4. Elle donne des résultats plus esthétiques qui ne sont
    pas vraiment utiles dans notre cas.
\end{itemize}

\subsection{Angle detection}

\subsubsection{Hough transform}

The Hough transform is a general method for finding lines and ellipses in an
image. It is often used because once the mathematical principle is understood,
it is not that hard to implement, and it has a much lower time complexity and
better results than some other methods like Fourier transform. For our purposes,
we only need to handle the most simple case: line detection in a binary image.\\

The Hough transform relies on a very particular representation of lines of the
plane: instead of being as the two parameters $a$ and $b$ in the classical
equation $y = ax + b$, it is described as the couple $(r, \theta)$ where $r$ is
the distance of the line from the origin and $\theta$ is the angle of this
distance from the absciss.\\

\begin{center}
\end{center}

How is this representation useful? It comes from the fact that, given a point
of coordinates $(x, y)$ and the angle $\theta$ of a line going through this
point, we can express very simply the distance $r$ of the line from the origin.
The equation is as follows:\\
>>>>>>> a594709738a33fb3f963fe16b8b134563e30681b

Une implémentation naïve commencerait par parcourir l'image d'origine et de transposer chaque pixel dans la nouvelle image grâce à une matrice de rotation. Cependant, cet algorithme donnera de très mauvais résultats puisque tous les pixels dans l'image crée n'aurons pas d'antécédants dans l'image d'origine, créant ainsi un effet d'aliasing.

A la place, nous effectuons l'opération inverse : pour chaque pixel de la nouvelle image (encore vide), il faut trouver le pixel correspondant dans l'image d'origine. Il suffit pour cela d'appliquer la matrice de rotation d'angle opposé.

A cause de l'imprecision des coordonnées entières, l'image de sortie n'est pas parfaite. C'est pourquoi nous avons utilisé un algorithme d'interpolation bilinéaire pour conserver la forme des caractères dans l'image avant de commencer la segmentation et l'identification des caractères.

\subsection{Interpolation Bilinéaire}
\subsection{Detection de l'angle}
\subsubsection{Transformée de Hough}

La transformée de Hough est une méthode générale pour trouver des lignes et des ellipses dans une image. Elle est souvent utilisée car une fois le principe mathématique compris, elle est simple à implémenter, donne de bons résultats et à une complexité assez faible par rapport, par exemple, à la transformée de Fourier. Nous n'avons besoin que du cas le plus simple de l'algorithme qui consiste à trouver les lignes dans une image binarisée.

La transformée de Hough se base sur une représentation très particulière des droites du plan : plutot que deux les représenter avec les deux paramètres usuels $a$ et $b$ dans l'équation $y = ax + b$, une droite est décrite comme un couple $(r, \theta)$ où $r$ est la distance de la ligne à l'origine et $\theta$ est l'angle de cette distance par rapport à l'abscisse.

En quoi cette représentation est-elle utile? Cela vient du fait que, étant donné un point de coordonnées $(x, y)$ et l'angle $\theta$ de la droite passant par ce point, il est facile d'exprimer la distance $r$ entre la ligne et l'origine. L'équation est la suivante :\\
$r = x\cos\theta + y\sin\theta$\\
Donc pour un point de coordonnées $(x, y)$ on peut tracer le graphe de $r$ en fonction de $\theta$. Il est facile de deviner d'après l'équation que ce graphe sera une sinusoïde. Cette sinusoïde représente l'ensemble des droites passant par ce point. Si nous stockons ce graphe dans un accumulateur, et ajoutons dans cet accumulateur la sinusoïde correspondant à tous les autres points, on obtient un graphe tableau dans lequel la valeur la plus grande représentera les coordonnées de la droite (dans l'espace de Hough) passant par le plus de points possible.

\subsubsection{More preprocessing}

<<<<<<< HEAD
Il y'a encore un problème à régler avant de pouvoir appliquer la transformée de Hough à notre image scannée : Une ligne de texte n'apparait pas comme une ligne droite. Nous avons essayé d'appliquer directement à une image binarisée, mais la plupart du temps il trouvait ou bien la diagonale de l'image, ou bien une reliure particulièrement visible. Après quelques recherches, nous avons trouvé une solution : on détecte tout d'abord les blocs de pixels noirs connexes, qui ne sont pas toujours mais très souvent des caractères, et on les remplaces par leur point centrale. L'utilité est triple : rendre plus apparentes (du point de vue de la transformée de Hough) les lignes de texte, réduire considérablement la complexité de l'algorithme et reduire l'importance des gros blocs de pixels comme les tâches ou les reliures.
=======
There is still one thing to fix before applying the Hough transform to our
scanned image: A text doesn't appears as a straight line. We tried to apply the
Hough transform directly to a binarized document, but most of the time it would
find either the diagonal of the document (since the diagonal goes through more
black points than an horizontal or a vertical line), or the angle of a prominent
bookbinding or document border. After a few researches, we came with our own
solution: we first find `blocks' of pixels, defined as being a set of conjoint
pixels. We remove every pixel of this block and replace them by a single pixel
at the center of the block. There are two purposes to this algorithm: replacing
every character by its center to make text lines apparent, and reducing the
importance of large blocks of pixels as bookbindings and images in the Hough
transform. Here is a sample output:\\

\begin{figure}[h!]\
    \centering
    \caption{input image}
\end{figure}
\begin{figure}[h!]\
    \centering
    \caption{center of each block}
\end{figure}
\begin{figure}[h!]\
    \centering
    \caption{line found by the Hough transform}
\end{figure}
>>>>>>> a594709738a33fb3f963fe16b8b134563e30681b

This method has a major flaw that we wish to correct before the end of the
project: when the noise isn't reduced enough, every separate pixel in the
`noisy zone' is considered a block and gets a huge role in the Hough transform,
potentially leading the angle detection to fail completely. Every further step
of the OCR relies on a perfectly straight image, and it is therefore very
important that we correct this. \\
